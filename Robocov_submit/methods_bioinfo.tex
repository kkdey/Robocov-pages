\section{Methods}\label{sec:methods-materials}

Let $X_{N \times P}$ be a data matrix with $N$ samples and $P$ features, where some of the entries $X_{np}$ may be missing, denoted here by $\text{NA}$. Let $X^f$ denote the fully-observed version of the partially-observed data matrix\footnote{Note that  $X$ is a restriction of $X^f$ to the observed entries.} $X$. We assume that samples are independent and follow a Multivariate Normal distribution: i.e., 
$X^f_{n,*} \sim \text{MVN} ( 0, \Sigma )$ 
%\begin{equation}\label{eq:normal_dist}
%    X^f_{n,*} \sim \text{MVN} ( 0, \Sigma ) ~~~\text{and}~~~\Omega = \Sigma^{-1}
%\end{equation}
where $\Sigma_{P \times P}$ and $\Omega:=\Sigma^{-1}$ (also of size $P \times P$) denote the model covariance and inverse covariance matrices respectively. 
Based on the observed entries, we obtain a matrix $\hat{\Sigma}$ of pairwise covariances such that for all $i, j \in \{1, \ldots, P\}$:
\begin{equation}\label{eq:standard_cov}
   \hat{\Sigma}_{ij} := \frac{1}{n_{ij} - 1} \sum_{n: X_{ni} \neq \text{NA}, X_{nj} \neq \text{NA}} (X_{ni} - \bar{X}_{i})(X_{nj} - \bar{X}_{j})
\end{equation}
where, $\bar{X}_{k}$ denotes the sample mean of feature $k$ based on the observed entries; and $n_{ij}$ is the number of samples $n$ with non-missing entries in both features $i$ and $j$. Let $n_i$ denote the number of observed samples (i.e., not missing) for feature $i$. For our analysis, we will assume\footnote{If necessary, as a pre-processing step, we remove features so that the condition $n_{ij}>2$ is satisfied for all $i,j$.} that $n_{ij}>2$ for all $i,j$.
We note that the matrix of all pairwise covariance terms: $\hat{\Sigma} = ((\hat{\Sigma}_{ij}))$, as defined in~\eqref{eq:standard_cov}, need not be positive semidefinite due to the presence of missing values in the data matrix.

 \subsection{Robocov covariance estimator}\label{sec:cov-estimator}

%The general optimization framework for \Robocov{} covariance estimator is a regularized criterion of the form: 
%\begin{equation}\label{eq:opt1-general-RM}
% \begin{aligned}
% & \text{min} 
%    &&  \mathcal{L} (\Sigma) + \lambda \xi (\Sigma)
% \end{aligned}
% \end{equation}
% where $\mathcal{L}(\Sigma)$ is the data fidelity term and $\xi(\Sigma)$ is the regularizer (or penalty function) encoding our prior beliefs about the estimator $\Sigma$.
%$\xi(\Sigma)$ represent the  data fidelity function and the penalty function respectively. 
%We present different estimators that are instances of Problem~\eqref{eq:opt1-general-RM} --- they depend upon different choices of the loss function and the regularizer. 

We first present the \Robocov{} covariance matrix estimator---this leads to an estimate of $\Sigma$ via the following regularized criterion: 
\begin{equation}\label{eq:opt1-RM}
 %%\begin{aligned}
    \text{min} ~ \sum_{i<j} |\Sigma_{ij}|~~
    ~~\text{s.t.}~~~~\Sigma \succeq 0,~~
   % &&& \hat{\Sigma} = \text{pairwise sample covariance} \\
   | \hat{\Sigma}_{ij} - \Sigma_{ij} |  \leq  C_{ij},  ~\forall i,j
%% \end{aligned}
\end{equation}
where $\Sigma$ is the optimization variable and $C_{ij}$s are data-driven constants that control the amount by which $\Sigma_{ij}$ can differ from the sample version $\hat{\Sigma}_{ij}$. Problem~\eqref{eq:opt1-RM} minimizes a convex penalty function (this encourages sparsity~\cite{hastie2015statistical} in $\Sigma_{ij}$s) subject to convex constraints (note that $\Sigma$ is positive-semidefinite i.e., $\Sigma \succeq 0$). Problem~\eqref{eq:opt1-RM} is a convex semidefinite optimization problem~\cite{BV2004}; and can be solved efficiently by modern semidefinite optimization algorithms for moderately large instances (e.g, $P \sim 1000$) using (for example) the SCS solver in CVX software~\cite{BV2004,o2016conic,Boyd2004, Fu2017}.


%Note that Problem~\eqref{eq:opt1-RM} minimizes a convex penalty function subject to convex constraints --- the optimization variable $\Sigma$ is positive-semidefinite (denoted as $\Sigma \succeq 0$). Hence~\eqref{eq:opt1-RM} is a convex semidefinite optimization problem~\cite{BV2004}; and can be solved efficiently by modern semidefinite optimization algorithms for moderately large instances (e.g, $P \sim 1000$) using (for example) the SCS solver in CVX software \cite{BV2004,o2016conic,Boyd2004, Fu2017}.
%\textcolor{red}{(basically, we can solve this in CVXR; or we can use SCS solver to solve p = 1000-ish problems)---not sure where to write this. At least we should cite some papers here. Some papers are~\cite{o2016conic,BV2004}.} 
%%%$\hat{\Sigma}$ is the pairwise covariance matrix~\eqref{eq:standard_cov}, and %%$\Sigma \succeq 0$ denotes that $\Sigma$ is positive-semi-definite. 
%The objective function in~\eqref{eq:opt1-RM} minimizes the $\ell_{1}$-norm on the entries of $\Sigma$; and induces sparsity in the solution \cite{hastie2015statistical}. 
%sparsity on the solution and the penalty function imposes 
%The constraint $|\hat{\Sigma}_{ij} - \Sigma_{ij}| \leq C_{ij}$ for all $i,j$
%is the data-fidelity term --- it constrains the entries of the estimated covariance matrix (i.e., $\Sigma_{ij}$) to be close to the sample covariance $\hat{\Sigma}_{ij}$---that is, 
%$\Sigma_{ij} \in [\hat{\Sigma}_{ij} - C_{ij}, \hat{\Sigma}_{ij} + C_{ij}]$. Here, $C_{ij}$ controls the amount by which $\Sigma_{ij}$ can differ from 
%the sample version $\hat{\Sigma}_{ij}$. 




We compute $C_{ij}$ based on the Fisher's Z-Score~\cite{fisher1915, fisher1921} (for a complete derivation see Supplementary Note):
\begin{equation}\label{eq:defineC}
\begin{aligned}
    C_{ij} & = \hat{\sigma}_{i} \hat{\sigma}_{j} \min \left (2,  \eta (n_{ij})  \left \{ 3 (1 - \hat{R}^2_{ij}) + 2 \sqrt{3} \eta (n_{ij}) \right \} \right )
    %\\
    %& \eta(n_{ij})  = \sqrt{\frac{1}{n_{ij} - 1} + \frac{2}{(n_{ij} - 1)^2}}
\end{aligned}
\end{equation}
where  $\eta(n_{ij})  = \sqrt{{1}/{(n_{ij} - 1)} + {2}/{(n_{ij} - 1)^2}}$;
and $\hat{R}$ is the pairwise sample correlation matrix derived from $\hat{\Sigma}$

In summary, we note that our proposed \Robocov{} estimator does not impute missing values per-se --- it directly leads to an estimate for the covariance matrix $\Sigma$ while taking into  account the presence of missing-values in the data matrix.  

%We also explore some other choices of the loss function $\mathcal{L}$ that lead to similar results on real data. 

While~\eqref{eq:opt1-RM} leads to a covariance matrix estimator, this can be modified to deliver a correlation matrix instead of a covariance matrix: 
\begin{equation}\label{eq:opt1-RM-R}
 \begin{aligned}
    \text{min} ~~~& \sum_{i<j} |\mathcal{R}_{ij}| \\
    \text{s.t.}~~~& ~\mathcal{R} \succeq 0, \mathcal{R}_{ii} = 1, \forall i, ~~~ | \hat{R}_{ij} - \mathcal{R}_{ij} |  \leq  C^{(R)}_{ij}, ~\forall i,j
 \end{aligned}
\end{equation}
where $\mathcal{R}$ is the optimization variable and $C^{(R)}_{ij} = \frac{\hat{C}_{ij}}{\hat{\sigma}_{i} \hat{\sigma}_{j}}$. See the Supplementary Note for additional details. 

In the Supplementary Note, we present a  
framework given by the minimization of a regularized loss function that generalizes the 
estimator presented in~\eqref{eq:opt1-RM}. 
%%In practice, we found that many of these estimators lead to similar results on real %%datasets---therefore, in this paper, we focus our attention on the basic %%estimator~\eqref{eq:opt1-RM}. 

\subsection{Robocov inverse covariance estimator}\label{sec:invcov-estimator}
We present a regularized likelihood framework to estimate the inverse covariance matrix ($\Omega$) under a sparsity constraint.
%and subsequently obtain a partial correlation matrix. 
%%We use a regularized likelihood framework to estimate $\Omega$ under a %%sparsity constraint. An appealing aspect of our estimator is that 
Our optimization criterion is 
convex in $\Omega$ (and not $\Sigma$ which was the case in Section~\ref{sec:cov-estimator}).


Recall that GLASSO minimizes an $\ell_{1}$-norm regularized negative log-likelihood criterion (fully observed case); and is given by:
\begin{equation}\label{glasso-problem-1}
\min_{\Omega \succ 0}~~~ -\log\det(\Omega) + \langle \tilde{\Sigma}, \Omega \rangle + \lambda \sum_{ij} |\Omega_{ij}|
\end{equation}
where, $L(\Omega; \tilde{\Sigma}):=-\log\det(\Omega) + \langle \tilde{\Sigma}, \Omega \rangle$ is the negative log-likelihood (ignoring irrelevant constants), $\tilde{\Sigma}$ is the fully observed sample covariance matrix and $\lambda \geq 0$ is the regularization parameter. 
Replacing $\tilde{\Sigma}$ by the observed matrix $\hat{\Sigma}$ in~\eqref{glasso-problem-1} is problematic 
due to the error in estimating the pairwise covariances arising from the
missing values (different cell entries of the sample covariance matrix involve different effective sample sizes $n_{ij}$s leading to varying accuracies in estimating $\tilde{\Sigma}_{ij}$s). To account for this uncertainty, we use ideas from robust optimization~\cite{ben2009robust,bertsimas2011theory}---to the best of our knowledge, this approach has not been used earlier in the context of sparse inverse covariance estimation (in the presence of missing values).
%for the missing values by using polyhedral uncertainty sets for sparse $\Omega$ estimation; to the best of our knowledge, such an approach has not been attempted before. 
Our robust optimization approach minimizes the worst-case loss arising from the errors in estimating the cell entries $\tilde{\Sigma}_{ij}$s. This leads to a min-max optimization problem of the form:
\begin{equation}\label{eq:opt2-RM}
    \min_{\Omega \succeq 0} ~ \max_{\substack{\Delta \\ 
    |\Delta_{ij}| \leq D_{ij}, \forall i,j}}~ 
    \left \{ -\log \det( \Omega) + \langle\Omega, \hat{\Sigma} + \Delta \rangle \right\} + \lambda \sum_{ij} |\Omega_{ij}| .
\end{equation}
%As~\eqref{eq:opt2-RM} involves minimization of a pointwise maximum (over $\Delta$) of convex functions $\Omega \mapsto L(\Omega; \hat{\Sigma} + \Delta) + \lambda \sum_{ij} |\Omega_{ij}|$, Problem~\eqref{eq:opt2-RM} is 
which is convex~\cite{BV2004} in $\Omega$ (See Supplementary Note).
Convexity ensures that a global minimum to the problem can be obtained reliably---making our approach different from traditional missing data techniques based on the EM algorithm~\cite{Dempster1977} that often lead to complex nonconvex optimization tasks with multiple local solutions. 

In words, the inner maximization over $\Delta$ in Problem~\eqref{eq:opt2-RM} gives the largest (or worst-case) value of the negative log-likelihood---$\max_{\Delta} L(\Omega; \hat{\Sigma} + \Delta)$
where, $\Delta$ captures the uncertainty involved in estimating the entries of the sample covariance matrix $\tilde{\Sigma}$ due to the presence of missing values. 
The outer minimization problem (wrt $\Omega$) considers the minimum of the \emph{adjusted} loss function
%$\Omega \mapsto \max_{\Delta} L(\Omega; \hat{\Sigma} + \Delta)$, 
in addition to an $\ell_{1}$-penalization on $\Omega$ that encourages a 
sparse estimate of $\Omega$.

The so-called uncertainty set~\cite{bertsimas2011theory} in $\Delta$ is given by: $|\Delta_{ij}| \leq D_{ij}$ (for all $i,j$)
where, the upper bound $D_{ij}$ arises from a probability computation using the Fisher's Z-score criterion (see Supplementary Note):
%Similar to $C_{ij}$ in the previous section, we estimate $D_{ij}$ accounting for the missing entries in the data matrix $X$ as follows.
\begin{equation}\label{eq:defineD}
\begin{aligned}
    D_{ij} & = C_{ij} + \tilde{C}_{ij} \\
    \tilde{C}_{ij} & = \hat{\sigma}_{i} \hat{\sigma}_{j} \min \left \{2,  \eta (N) \left \{ 3 (1 - \hat{R}^2_{ij}) + 2 \sqrt{3} \eta (N) \right \} \right \}.
\end{aligned}
\end{equation}
Above, the value of the error $D_{ij}$ will be large if $n_{ij}$ is small, and equal to zero when $n_{ij}=n$ (with no missing entries). 

The seemingly complicated min-max optimization problem in~\eqref{eq:opt2-RM}
reduces to a cousin of the GLASSO criterion (See Supplementary Note for details) --- we use a weighted version of the $\ell_{1}$-norm penalty on $\Omega$: 
\begin{equation}\label{eq:opt3-RM}
    \begin{aligned}
    \min_{\Omega \succeq 0} ~~ & \left \{- \log \det ( \Omega ) + \langle \Omega, \hat{\Sigma} \rangle +  \sum_{ij} (\lambda +  D_{ij})|\Omega_{ij}|  \right \}.
    \end{aligned}
\end{equation}
Problem~\eqref{eq:opt3-RM} is a nonlinear semidefinite optimization problem in $\Omega$---and the constraint $\Omega \succeq 0$ leads to a positive semidefinite inverse covariance matrix\footnote{We get a positive semidefinite (PSD) estimate for $\Omega$ even if $\hat{\Sigma}$ is not PSD. The $\log\det$-term in the objective encourages an optimal solution to~\eqref{eq:opt3-RM} to be positive definite (i.e, of full rank).}.
Problem~\eqref{eq:opt3-RM} uses a weighted $\ell_{1}$-norm on $\Omega$ where the penalty weights are adjusted to account for the uncertainty due to the presence of missing values. Note that the penalty parameter $\lambda$ accounts for the sparsity in $\Omega$ arising from our prior sparsity assumption on $\Omega$---the overall penalty weight for the $(i,j)$-th entry: $(\lambda + D_{ij})$ adds further regularization due to the presence of missing values. 
%In particular, if there is no missing value, then $D_{ij}=0$ and~\eqref{eq:opt3-RM} will reduce to the GLASSO criterion. If $n_{ij}$ is small, then the value of $D_{ij}$ will be large --- therefore, we will place a higher weight on the term $|\Omega_{ij}|$ to shrink it towards zero. 
%That is, if the number of observations $n_{ij}$ is sufficiently small, 
%%%absence of sufficiently many samples,
%we prefer to have the partial covariance matrix entry ($\Omega_{ij}$ and $\Omega_{ji}$) to be close to zero. 
%Note that the above problem is convex. See Supplementary Note for the proof of this derivation. If $n_{ij}$ are large, then $|D_{ij}|$ would be close to $0$ and the optimization problem will reduce to the standard GLASSO optimization. The smaller the $n_{ij}$, the larger would be $D_{ij}$ which will enforce greater shrinkage on $\Sigma_{ij}$ towards $0$. 

Note that, as in Section~\ref{sec:cov-estimator}, the \Robocov{} inverse covariance estimator, bypasses the task of imputing the missing values. Our main goal is to directly estimate $\Omega$ from a partially observed data-matrix $X$. In this way, we can potentially mitigate the limitations of a sub-optimal imputation procedure. See Section~\ref{sec:results} for an empirical validation. 

%Criterion~\eqref{eq:opt3-RM} leads to an inverse covariance estimator --- we use 
The inverse covariance estimate $\Omega$ from~\eqref{eq:opt3-RM}, can be used to obtain the partial correlation estimator $\mathcal{W}$ as follows 

\begin{equation}\label{eq:define_partialcor}
    \mathcal{W}_{ij} := -\frac{\Omega_{ij}}{\sqrt{\Omega_{ii} \Omega_{jj}}}.
\end{equation}

%Both the optimization problems~\eqref{eq:opt1-RM} and 
Problem~\eqref{eq:opt3-RM} was solved using R implementation of the CVX software \cite{Boyd2004, Fu2017}. This was sufficient for the problem-scales we are dealing with.
%--- for larger instances, specialized algorithms~\cite{friedman2008,mazumder2012graphical} may be necessary.

In all our subsequent analysis and numerical results, we use the \Robocov{} correlation estimator $\mathcal{R}$ (see Problem~\eqref{eq:opt1-RM-R}) and partial correlation estimator $\mathcal{W}$~\eqref{eq:define_partialcor}.




%%%%%%%%%%%%%%%%%%%%% Older version by KKDey
\begin{comment}
Our inverse covariance estimator builds upon the popular $\ell_{1}$-regularized Gaussian likelihood
framework (aka graphical lasso or GLASSO~\cite{friedman2008, witten2011, mazumder2012}) for the fully observed case, and 
adapts it to address missing values. The GLASSO optimization criterion is given by:
$$\min_{\Omega \succ 0}~~ \underbrace{-\log\det(\Omega) + \langle \tilde{\Sigma}, \Omega \rangle}_{:=L(\Omega; \tilde{\Sigma})} + \lambda \sum_{ij} |\Omega_{ij}|$$
where $\tilde{\Sigma}$ is the fully observed sample covariance matrix and $\lambda \geq 0$ is the regularization parameter. 

Replacing $\tilde{\Sigma}$ by the observed matrix $\hat{\Sigma}$ in $L(\Omega; \tilde{\Sigma})$ because of missing data leading to differences in $n_{ij}$s. To account for this uncertainty, we introduce a novel robust optimization approach~\cite{ben2009robust,bertsimas2011theory} that minimizes the worst-case loss arising from the errors in estimating the cell entries $\tilde{\Sigma}_{ij}$s. This leads to a min-max optimization problem of the form:
\begin{equation}\label{eq:opt2-RM}
    \min_{\Omega \succeq 0} ~~ \max_{\Delta: |\Delta_{ij}| \leq D_{ij}} ~~ 
    \left \{ -\log \det( \Omega) + \langle\Omega, \hat{\Sigma} + \Delta \rangle \right\} + \lambda \sum_{ij} |\Omega_{ij}| .
\end{equation}
As~\eqref{eq:opt2-RM} involves minimization of a pointwise maximum (over $\Delta$) of convex functions $\Omega \mapsto L(\Omega; \hat{\Sigma} + \Delta) + \lambda \sum_{ij} |\Omega_{ij}|$, Problem~\eqref{eq:opt2-RM} is convex~\cite{BV2004} in $\Omega$.

The so-called uncertainty set~\cite{bertsimas2011theory} in $\Delta$ is given by: $|\Delta_{ij}| \leq D_{ij}$ (for all $i,j$)
where, the upper bound $D_{ij}$ arises from a probability computation using the Fisher's Z-score criterion (see Supplementary Note):
%Similar to $C_{ij}$ in the previous section, we estimate $D_{ij}$ accounting for the missing entries in the data matrix $X$ as follows.
\begin{equation}\label{eq:defineD}
\begin{aligned}
    D_{ij} & = C_{ij} + \tilde{C}_{ij} \\
    \tilde{C}_{ij} & = \hat{\sigma}_{i} \hat{\sigma}_{j} \min \left \{2,  \eta (N) \left \{ 3 (1 - \hat{R}^2_{ij}) + 2 \sqrt{3} \eta (N) \right \} \right \}.
\end{aligned}
\end{equation}
The value of the error $D_{ij}$ will be large if $n_{ij}$ is small, and will be equal to zero when $n_{ij}=n$ (with no missing entries). 

The optimization problem in~\eqref{eq:opt2-RM}
reduces to a cousin of the GLASSO criterion (Supplementary Note): 
\begin{equation}\label{eq:opt3-RM}
    \begin{aligned}
    \min_{\Omega \succeq 0} ~~ & \left \{- \log \det ( \Omega ) + \langle \Omega, \hat{\Sigma} \rangle +  \sum_{ij} (\lambda +  D_{ij})|\Omega_{ij}|  \right \}.
    \end{aligned}
\end{equation}
Problem~\eqref{eq:opt3-RM} is a nonlinear semidefinite optimization problem in $\Omega$---and the constraint $\Omega \succeq 0$ and the presence of the $\log\det$ in the objective lead to a positive semi definite (PSD) solution for $\Omega$ even if $\hat{\Sigma}$ is not PSD. Both the optimization problems~\eqref{eq:opt1-RM} and \eqref{eq:opt3-RM} were solved using the SCS solver in the R implementation of the CVX software \cite{Boyd2004, Fu2017}.

%Problem~\eqref{eq:opt3-RM} is a nonlinear semidefinite optimization problem in $\Omega$---and the constraint $\Omega \succeq 0$ leads to a positive semidefinite inverse covariance matrix\footnote{We get a positive semidefinite (PSD) estimate for $\Omega$ even if $\hat{\Sigma}$ is not PSD. In addition, due to the presence of the $\log\det$ in the objective, an optimal solution to~\eqref{eq:opt3-RM} will be positive definite (i.e, $\Omega$ will have full rank).}.
%AProblem~\eqref{eq:opt3-RM} uses a weighted $\ell_{1}$-norm on $\Omega$ where the penalty weights are adjusted to account for the uncertainty due to the presence of missing values. Note that the penalty parameter $\lambda$ accounts for the sparsity in $\Omega$ arising from our prior sparsity assumption on $\Omega$---the overall penalty weight for the $(i,j)$-th entry, $(\lambda + D_{ij})$ adds further regularization due to the presence of missing values. 
%In particular, if there is no missing value, then $D_{ij}=0$ and~\eqref{eq:opt3-RM} will reduce to the GLASSO criterion. If $n_{ij}$ is small, then the value of $D_{ij}$ will be large --- therefore, we will place a higher weight on the term $|\Omega_{ij}|$ to shrink it towards zero.

%Problem~\eqref{eq:opt3-RM} uses a weighted $\ell_{1}$-norm on $\Omega$ where the penalty weights are adjusted to account for the uncertainty due to the presence of missing values. Note that the penalty parameter $\lambda$ accounts for the sparsity in $\Omega$ arising from our prior sparsity assumption on $\Omega$---the overall penalty weight for the $(i,j)$-th entry, $(\lambda + D_{ij})$ adds further regularization due to the presence of missing values. 
%In particular, if there is no missing value, then $D_{ij}=0$ and~\eqref{eq:opt3-RM} will reduce to the GLASSO criterion. If $n_{ij}$ is small, then the value of $D_{ij}$ will be large --- therefore, we will place a higher weight on the term $|\Omega_{ij}|$ to shrink it towards zero. 

Like in in Section~\ref{sec:cov-estimator}, the \Robocov{} inverse covariance estimator bypasses the task of imputing the missing values. Our main goal is to directly estimate $\Omega$ from a partially observed data-matrix $X$. In this way, we can potentially mitigate the limitations of a sub-optimal imputation procedure. See Section~\ref{sec:results} for an empirical validation. We use the solution $\Omega$ from Problem~\eqref{eq:opt3-RM} to define a partial correlation estimator $\mathcal{W}$ as follows 

\begin{equation}\label{eq:define_partialcor}
    \mathcal{W}_{ij} := -\frac{\Omega_{ij}}{\sqrt{\Omega_{ii} \Omega_{jj}}}.
\end{equation}

In all our subsequent analysis and numerical results, we use the \Robocov{} correlation estimator $\mathcal{R}$ (see Problem~\eqref{eq:opt1-RM-R}) and partial correlation estimator $\mathcal{W}$~\eqref{eq:define_partialcor}.
\end{comment}



