\section{Methods}\label{sec:methods-materials}

Let $X_{N \times P}$ be a data matrix with $N$ samples and $P$ features, where some of the entries $X_{np}$ may be missing, denoted here by $\text{NA}$. 
%We assume that each row and each column of $X$ has at one observed entry. 
We let $X^f$ denote the fully-observed version of the partially-observed data matrix\footnote{Note that the data matrix $X$ is a restriction of $X^f$ to the observed entries.} $X$.
We further assume that the fully observed data vectors $X^f_{n,*}$, $n=1, \ldots, N$ are independent and follow a Multivariate Normal distribution:
\begin{equation}\label{eq:normal_dist}
    X^f_{n,*} \sim \text{MVN} \left ( 0, \Sigma \right ) \hspace{1 cm} \Omega = \Sigma^{-1}
\end{equation}
where $\Sigma_{P \times P}$ and $\Omega_{P \times P}$ denote the model covariance and the inverse covariance matrices respectively. 
Based on the observed entries, we obtain a matrix $\hat{\Sigma}$ of pairwise covariances such that for all $i, j \in \{1, \ldots, P\}$:
\begin{equation}\label{eq:standard_cov}
   \hat{\Sigma}_{ij} := \frac{1}{n_{ij} - 1} \sum_{n: X_{ni} \neq \text{NA}, X_{nj} \neq \text{NA}} (X_{ni} - \bar{X}_{i})(X_{nj} - \bar{X}_{j})
\end{equation}
where, $\bar{X}_{k}$ denotes the sample mean of feature $k$ based on the observed entries; and $n_{ij}$ is the number of samples $n$ with non-missing entries in both features $i$ and $j$: 
\begin{equation}\label{eq:missing_n}
\bar{X}_{k} = \frac{1}{n_{k}} \sum_{n: X_{nk} \neq \text{NA}} X_{nk},~~~~~n_{ij} : = \# \left \{n: \; X_{ni} \neq \text{\text{NA}},  \;\; X_{nj} \neq \text{\text{NA}} \right \}.
\end{equation}
Here $n_k$ denotes the number of observed samples (i.e., not missing) for feature $k$. For our analysis, we will assume\footnote{If necessary, as a pre-processing step, we remove features so that the condition $n_{ij}>2$ is satisfied for all $i,j$.} that $n_{ij}>2$ for all $i,j$.
We note that the matrix of all pairwise covariance terms: $\hat{\Sigma} = ((\hat{\Sigma}_{ij}))$, as defined in~\eqref{eq:standard_cov}, need not be positive semidefinite due to the presence of missing values in the data matrix.

\subsection{Robocov covariance estimator}\label{sec:cov-estimator}

%The general optimization framework for \Robocov{} covariance estimator is a regularized criterion of the form: 
%\begin{equation}\label{eq:opt1-general-RM}
% \begin{aligned}
% & \text{min} 
%    &&  \mathcal{L} (\Sigma) + \lambda \xi (\Sigma)
% \end{aligned}
% \end{equation}
% where $\mathcal{L}(\Sigma)$ is the data fidelity term and $\xi(\Sigma)$ is the regularizer (or penalty function) encoding our prior beliefs about the estimator $\Sigma$.
%$\xi(\Sigma)$ represent the  data fidelity function and the penalty function respectively. 
%We present different estimators that are instances of Problem~\eqref{eq:opt1-general-RM} --- they depend upon different choices of the loss function and the regularizer. 

We first present the \Robocov{} covariance matrix estimator---this leads to an estimate of $\Sigma$ via the following regularized criterion: 
\begin{equation}\label{eq:opt1-RM}
 \begin{aligned}
    \text{min} ~ \sum_{i<j} |\Sigma_{ij}|~~
    ~~\text{s.t.}~~~~\Sigma \succeq 0,~~
   % &&& \hat{\Sigma} = \text{pairwise sample covariance} \\
   | \hat{\Sigma}_{ij} - \Sigma_{ij} |  \leq  C_{ij},  ~\forall i,j
 \end{aligned}
\end{equation}
where $\Sigma$ is the optimization variable and $\hat{\Sigma}$ and $C_{ij}$s are problem data. Note that Problem~\eqref{eq:opt1-RM} minimizes a convex penalty function subject to convex constraints --- the optimization variable $\Sigma$ is positive-semidefinite (denoted as $\Sigma \succeq 0$). Hence~\eqref{eq:opt1-RM} is a convex semidefinite optimization problem~\cite{BV2004}; and can be solved efficiently by modern semidefinite optimization algorithms for moderately large instances (e.g, $P \sim 1000$) using (for example) the SCS solver in CVX software \cite{BV2004,o2016conic,Boyd2004, Fu2017}.
%\textcolor{red}{(basically, we can solve this in CVXR; or we can use SCS solver to solve p = 1000-ish problems)---not sure where to write this. At least we should cite some papers here. Some papers are~\cite{o2016conic,BV2004}.} 
%%%$\hat{\Sigma}$ is the pairwise covariance matrix~\eqref{eq:standard_cov}, and %%$\Sigma \succeq 0$ denotes that $\Sigma$ is positive-semi-definite. 
The objective function in~\eqref{eq:opt1-RM} minimizes the $\ell_{1}$-norm on the entries of $\Sigma$; and induces sparsity in the solution \cite{hastie2015statistical}. 
%sparsity on the solution and the penalty function imposes 
The constraint $|\hat{\Sigma}_{ij} - \Sigma_{ij}| \leq C_{ij}$ for all $i,j$
is the data-fidelity term --- it constrains the entries of the estimated covariance matrix (i.e., $\Sigma_{ij}$) to be close to the sample covariance $\hat{\Sigma}_{ij}$---that is, 
$\Sigma_{ij} \in [\hat{\Sigma}_{ij} - C_{ij}, \hat{\Sigma}_{ij} + C_{ij}]$. Here, $C_{ij}$ controls the amount by which $\Sigma_{ij}$ can differ from 
the sample version $\hat{\Sigma}_{ij}$. 
%an upper bound $C$ on the difference between pairwise sample covariance %estimator  $\hat{\Sigma}$ and population covariance matrix $\Sigma$, %accounting for the missing-ness in data matrix $X$.
%We estimate the upper bound $C$ as follows.
We compute $C_{ij}$ based on the Fisher's Z-Score~\cite{fisher1915, fisher1921} (for a complete derivation see Supplementary Note):
\begin{equation}\label{eq:defineC}
\begin{aligned}
    C_{ij} & = \hat{\sigma}_{i} \hat{\sigma}_{j} \min \left (2,  \eta (n_{ij})  \left \{ 3 (1 - \hat{R}^2_{ij}) + 2 \sqrt{3} \eta (n_{ij}) \right \} \right ) \\
    & \eta(n_{ij})  = \sqrt{\frac{1}{n_{ij} - 1} + \frac{2}{(n_{ij} - 1)^2}}
\end{aligned}
\end{equation}
where $\hat{R}$ is the pairwise sample correlation matrix derived from $\hat{\Sigma}$ i.e., 
\begin{equation}\label{eq:defineRhat}
\hat{R}_{ij} = \frac{\hat{\Sigma}_{ij}}{\hat{\sigma}_{i}\hat{\sigma}_{j}}~\forall i,j ~~\text{and}~~~\hat{\sigma}_{i} = \hat{\Sigma}_{ii}~\forall i.
\end{equation}

%and $\hat{\sigma}_{k} = \hat{\Sigma}_{kk}$. 
%See Supplementary Note for derivation of $C$. 

%\textcolor{red}{some of the details below can be placed in the appendix. However, I think there should be some discussion around the regularized loss + penalty formulation here. This will also allow for the introduction of a tuning parameter. It is somewhat odd to have an estimator w/o tuning parameters. The tuning parameter in the appendix is hidden in the probability statement or epsilon which you have chosen.}
Note that criterion~\eqref{eq:opt1-RM} 
%involves the minimization of a penalty for $\Sigma$, subject to data-fidelity constraints; and does
%\emph{not} involve any tuning parameters. This 
can be perceived as a special case of a more general regularized optimization problem
\begin{equation}\label{eq:opt1-RM-Gen}
    \begin{aligned}
     \text{min} ~ {\mathcal L}(\Sigma; \hat{\Sigma}) +  \lambda \sum_{i<j} |\Sigma_{ij}|
    ~~~~~\text{s.t.}~~~~\Sigma \succeq 0
    \end{aligned}
\end{equation}
where, ${\mathcal L}(\Sigma; \hat{\Sigma})$ is the data-fidelity term or loss function measuring the proximity of $\Sigma$ to $\hat{\Sigma}$; the second term represents the regularization on $\Sigma$; and $\lambda$ is a tuning parameter that controls the trade-off between data-fidelity and regularization. We can choose ${\mathcal L}(\Sigma; \hat{\Sigma}) = \sum_{ij} {\mathcal L}_{ij}(\Sigma_{ij}, \hat{\Sigma}_{ij})$ with 
${\mathcal L}_{ij}(\Sigma_{ij}, \hat{\Sigma}_{ij})= \max \{| \hat{\Sigma}_{ij} - \Sigma_{ij} |  -  C_{ij}, 0\}$ for all $i,j$.
This leads to a regularized convex optimization problem of the form:
\begin{equation}\label{eq:opt1-RM-Gen1}
    \begin{aligned}\min~~ \frac{1}{\lambda}{\mathcal L}(\Sigma; \hat{\Sigma}) +  \sum_{i<j} |\Sigma_{ij}|.
    \end{aligned}
\end{equation}
In the limiting case, $\lambda\rightarrow 0+$ i.e., $1/\lambda \rightarrow \infty$, estimator obtained from Problem~\eqref{eq:opt1-RM-Gen1} will reduce to the estimator available from~\eqref{eq:opt1-RM}. This is because, for sufficiently large values of $1/\lambda$, 
an optimal solution to~\eqref{eq:opt1-RM-Gen1} will lead to a zero loss---${\mathcal L}(\Sigma; \hat{\Sigma}) = 0$ which implies that 
${\mathcal L}_{ij}(\Sigma_{ij}; \hat{\Sigma}_{ij}) = 0$ for all $i,j$ --- these are the data-fidelity constraints in~\eqref{eq:opt1-RM}.

In summary, we note that our proposed \Robocov{} estimator does not impute missing values per-se --- it directly leads to an estimate for the covariance matrix $\Sigma$ while taking into  account the presence of missing-values in the data matrix.  

Other choices of the loss function ${\mathcal L}$ are also possible --- we present one such candidate in the Supplementary Note that is also derived from the Fisher's Z-Score setup (which was key to deriving~\eqref{eq:opt1-RM}). In practice, we found that many of these estimators lead to similar results on real datasets---therefore, in this paper, we focus our attention on the basic estimator~\eqref{eq:opt1-RM}. 

\smallskip

\noindent {\bf{From covariance to correlation estimates}:} 
Formulation~\eqref{eq:opt1-RM} delivers an estimate of the covariance matrix.
To obtain a correlation matrix estimate, 
%One can estimate the \Robocov{} correlation estimator by either re-scaling the \Robocov{} covariance estimator obtained from Problem~\eqref{eq:opt1-RM}. Another possibility is via 
one can modify~\eqref{eq:opt1-RM}
to deliver a correlation matrix instead of a covariance matrix: 
\begin{equation}\label{eq:opt1-RM-R}
 \begin{aligned}
    \text{min} ~ \sum_{i<j} |\mathcal{R}_{ij}|~~
    ~~\text{s.t.}~~\mathcal{R} \succeq 0, \mathcal{R}_{ii} = 1, \forall i,~~
   % &&& \hat{\Sigma} = \text{pairwise sample covariance} \\
   | \hat{R}_{ij} - \mathcal{R}_{ij} |  \leq  C^{(R)}_{ij}, ~\forall i,j
 \end{aligned}
\end{equation}
where $\mathcal{R}$ is the optimization variable, $\hat{R}$ is
defined in~\eqref{eq:defineRhat} and $C^{(R)}_{ij} = \frac{\hat{C}_{ij}}{\hat{\sigma}_{i} \hat{\sigma}_{j}}$. The derivation of $C^{(R)}$ follows from the derivation of $C$ and appears in the Supplementary Note.

A simple alternative approach to obtain the \Robocov{} correlation matrix estimate, is to re-scale the \Robocov{} covariance estimator (obtained from Problem~\eqref{eq:opt1-RM}) to a correlation matrix. 




\subsection{Robocov inverse covariance estimator}\label{sec:invcov-estimator}

Section~\ref{sec:cov-estimator} discusses a convex optimization-based estimator for the covariance matrix ($\Sigma$), here we present a method to estimate the inverse covariance matrix ($\Omega$) and subsequently the  partial correlation matrix. 
We present a regularized likelihood framework to estimate $\Omega$ under a sparsity constraint. An appealing aspect of our estimator is that our optimization criterion is 
convex in $\Omega$ (and not $\Sigma$ which was the case in Section~\ref{sec:cov-estimator}). 

Our estimator builds upon the popular $\ell_{1}$-regularized Gaussian likelihood
framework (aka graphical lasso or GLASSO\cite{friedman2008, witten2011, mazumder2012}) for the fully observed case, and 
adapts it to address missing values. We recall the GLASSO procedure which minimizes an $\ell_{1}$-norm regularized negative log-likelihood criterion (fully observed case) given by:
$$\min_{\Omega \succ 0}~~ \underbrace{-\log\det(\Omega) + \langle \tilde{\Sigma}, \Omega \rangle}_{:=L(\Omega; \tilde{\Sigma})} + \lambda \sum_{ij} |\Omega_{ij}|$$
where, $L(\Omega; \tilde{\Sigma})$ is the negative log-likelihood (ignoring irrelevant constants) for the model~\eqref{eq:normal_dist}, $\tilde{\Sigma}$ is the fully observed sample covariance matrix and $\lambda \geq 0$ is the regularization parameter. 

Replacing $\tilde{\Sigma}$ by the observed matrix $\hat{\Sigma}$ in $L(\Omega; \tilde{\Sigma})$ is problematic 
due to the error in estimating the pairwise covariances arising from the
missing values (different cell entries of the sample covariance matrix involve different effective sample sizes $n_{ij}$s leading to varying accuracies in estimating $\tilde{\Sigma}_{ij}$s). To account for this uncertainty, we use ideas from robust optimization~\cite{ben2009robust,bertsimas2011theory}---to the best of our knowledge, this approach has not been used earlier in the context of sparse inverse covariance estimation (in the presence of missing values).
%for the missing values by using polyhedral uncertainty sets for sparse $\Omega$ estimation; to the best of our knowledge, such an approach has not been attempted before. 
Our robust optimization approach minimizes the worst-case loss arising from the errors in estimating the cell entries $\tilde{\Sigma}_{ij}$s. This leads to a min-max optimization problem of the form:
\begin{equation}\label{eq:opt2-RM}
    \min_{\Omega \succeq 0} ~~ \max_{\Delta: |\Delta_{ij}| \leq D_{ij}} ~~ 
    \left \{ -\log \det( \Omega) + \langle\Omega, \hat{\Sigma} + \Delta \rangle \right\} + \lambda \sum_{ij} |\Omega_{ij}| .
\end{equation}
As~\eqref{eq:opt2-RM} involves minimization of a pointwise maximum (over $\Delta$) of convex functions $\Omega \mapsto L(\Omega; \hat{\Sigma} + \Delta) + \lambda \sum_{ij} |\Omega_{ij}|$, Problem~\eqref{eq:opt2-RM} is convex~\cite{BV2004} in $\Omega$.
Convexity ensures that a global minimum to the problem can be obtained reliably---making our approach different from traditional missing data techniques based on the EM algorithm\cite{Dempster1977} that often lead to complex nonconvex optimization tasks with multiple local solutions. 

In words, the inner maximization over $\Delta$ in Problem~\eqref{eq:opt2-RM} gives the largest (or worst-case) value of the negative log-likelihood---$\max_{\Delta} L(\Omega; \hat{\Sigma} + \Delta)$
where, $\Delta$ captures the uncertainty involved in estimating the entries of the sample covariance matrix $\tilde{\Sigma}$ due to the presence of missing values. 
The outer minimization problem (wrt $\Omega$) considers the minimum of the \emph{adjusted} worst-case loss function  $\Omega \mapsto \max_{\Delta} L(\Omega; \hat{\Sigma} + \Delta)$, in addition to an $\ell_{1}$-penalization on $\Omega$ that encourages a 
sparse estimate of $\Omega$.

The so-called uncertainty set~\cite{bertsimas2011theory} in $\Delta$ is given by: $|\Delta_{ij}| \leq D_{ij}$ (for all $i,j$)
where, the upper bound $D_{ij}$ arises from a probability computation using the Fisher's Z-score criterion (see Supplementary Note for additional details):
%Similar to $C_{ij}$ in the previous section, we estimate $D_{ij}$ accounting for the missing entries in the data matrix $X$ as follows.
\begin{equation}\label{eq:defineD}
\begin{aligned}
    D_{ij} & = C_{ij} + \tilde{C}_{ij} \\
    \tilde{C}_{ij} & = \hat{\sigma}_{i} \hat{\sigma}_{j} \min \left \{2,  \eta (N) \left \{ 3 (1 - \hat{R}^2_{ij}) + 2 \sqrt{3} \eta (N) \right \} \right \}.
\end{aligned}
\end{equation}
To provide some intuition about~\eqref{eq:defineD}, the value of the error $D_{ij}$ will be large if $n_{ij}$ is small, and will be equal to zero when $n_{ij}=n$ (with no missing entries). 

The seemingly complicated min-max optimization problem in~\eqref{eq:opt2-RM}
reduces to a cousin of the GLASSO criterion (See Supplementary Note for details) --- we use a weighted version of the $\ell_{1}$-norm penalty on $\Omega$: 
\begin{equation}\label{eq:opt3-RM}
    \begin{aligned}
    \min_{\Omega \succeq 0} ~~ & \left \{- \log \det ( \Omega ) + \langle \Omega, \hat{\Sigma} \rangle +  \sum_{ij} (\lambda +  D_{ij})|\Omega_{ij}|  \right \}.
    \end{aligned}
\end{equation}
Problem~\eqref{eq:opt3-RM} is a nonlinear semidefinite optimization problem in $\Omega$---and the constraint $\Omega \succeq 0$ leads to a positive semidefinite inverse covariance matrix\footnote{We get a positive semidefinite (PSD) estimate for $\Omega$ even if $\hat{\Sigma}$ is not PSD. In addition, due to the presence of the $\log\det$ in the objective, an optimal solution to~\eqref{eq:opt3-RM} will be positive definite (i.e, 
$\Omega$ will have full rank).}.
Problem~\eqref{eq:opt3-RM} uses a weighted $\ell_{1}$-norm on $\Omega$ where the penalty weights are adjusted to account for the uncertainty due to the presence of missing values. Note that the penalty parameter $\lambda$ accounts for the sparsity in $\Omega$ arising from our prior sparsity assumption on $\Omega$---the overall penalty weight for the $(i,j)$-th entry, $(\lambda + D_{ij})$ adds further regularization due to the presence of missing values. 
In particular, if there is no missing value, then $D_{ij}=0$ and~\eqref{eq:opt3-RM} will reduce to the GLASSO criterion. If $n_{ij}$ is small, then the value of $D_{ij}$ will be large --- therefore, we will place a higher weight on the term $|\Omega_{ij}|$ to shrink it towards zero. 
%That is, if the number of observations $n_{ij}$ is sufficiently small, 
%%%absence of sufficiently many samples,
%we prefer to have the partial covariance matrix entry ($\Omega_{ij}$ and $\Omega_{ji}$) to be close to zero. 
%Note that the above problem is convex. See Supplementary Note for the proof of this derivation. If $n_{ij}$ are large, then $|D_{ij}|$ would be close to $0$ and the optimization problem will reduce to the standard GLASSO optimization. The smaller the $n_{ij}$, the larger would be $D_{ij}$ which will enforce greater shrinkage on $\Sigma_{ij}$ towards $0$. 

Note that, as in Section~\ref{sec:cov-estimator}, the \Robocov{} inverse covariance estimator, bypasses the task of imputing the missing values. Our main goal is to directly estimate $\Omega$ from a partially observed data-matrix $X$. In this way, we can potentially mitigate the limitations of a sub-optimal imputation procedure. See Section~\ref{sec:results} for an empirical validation. 

Criterion~\eqref{eq:opt3-RM} leads to an inverse covariance estimator --- we use the solution $\Omega$ from Problem~\eqref{eq:opt3-RM} to define a partial correlation estimator $\mathcal{W}$ as follows 

\begin{equation}\label{eq:define_partialcor}
    \mathcal{W}_{ij} := -\frac{\Omega_{ij}}{\sqrt{\Omega_{ii} \Omega_{jj}}}.
\end{equation}

Both the optimization problems~\eqref{eq:opt1-RM} and \eqref{eq:opt3-RM} were solved using R implementation of the CVX software \cite{Boyd2004, Fu2017}. This was sufficient for the problem-scales we are dealing with --- for larger instances, specialized algorithms (e.g., based on first order methods)~\cite{hastie2015statistical,mazumder2012graphical,atchade2015scalable} may be necessary.

In all our subsequent analysis and numerical results, we use the \Robocov{} correlation estimator $\mathcal{R}$ (see Problem~\eqref{eq:opt1-RM-R}) and partial correlation estimator $\mathcal{W}$~\eqref{eq:define_partialcor}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%END%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%Additionally, one can also use an adaptive version of the optimization problem \ref{eq:opt1} along the lines of \cite{}. 

\begin{comment}

Define $\mathcal{G}(\Sigma)$ to be the optimizing function from Equation \ref{eq:nuclearopt}
\begin{equation} \label{eq:defG}
    \mathcal{G}(\Sigma) := \tilde{L} (\Sigma) + \lambda ||\Sigma||_{\star}
\end{equation}


Then show that the gradient of $\mathcal{G}$ satisfies the Lipschitz condition
\begin{equation}\label{eq:lipschitzG}
    ||  \nabla \mathcal{G} (\Sigma_1) - \nabla \mathcal{G} (\Sigma_2) || < \kappa || \Sigma_1 - \Sigma_2 || 
\end{equation}

We use a projected gradient descent (GD) algorithm to solve for $\Sigma$. At iteration $k+1$, our update $\Sigma^{(k+1)}$ will follow 

\begin{equation}\label{eq:gd}
\Sigma^{(k+1)} = \underset{\Sigma \geq 0 } {arg min} \left |\left | \Sigma - \left (\Sigma^{(k)} - \eta \nabla \mathcal{G} (\Sigma^{(k)}) \right ) \right |\right|^2_{F} 
\end{equation}

To solve this, we first consider eigen-decomposition of the gradient descent update

\begin{equation}\label{eq:gradupdate}
\Sigma^{(k)} - \eta \nabla \mathcal{G} (\Sigma^{(k)}) = UDU^{T} 
\end{equation}

and then project it to the space of positive semi-definite matrices by setting all negative eigenvalues to 0, so 

\begin{equation}\label{eq:thetak+1}
 \Sigma^{(k+1)} = UD^{+}U^{T} \hspace{1 cm} D^{+}_{ii} = max(0, D_{ii})   
\end{equation}


\end{comment}


